#!/bin/bash
# this script is part of https://github.com/WolfgangFahl/ProceedingsTitleParser
# it gets the test data sets for
#
# 1. CEUR-WS
#
# WF 2020-07-01

sampledir="sampledata"

#
# download
#
download() {
  local l_src="$1"
  local l_target="$2"
  if [ ! -f $l_target ]
  then
    echo "downloading $l_target from $l_src"
    curl -o $l_target $l_src
  else
    echo "$l_target already downloaded"
  fi
  wc -l $l_target
}

#
# get the data from CEUR-WS
#
getCEURWS() {
  echo "getting sample data for CEUR-WS"
  samplehtml=$sampledir/ceurs-ws.html
  sampletxt=$sampledir/proceedings-ceur-ws.txt
  download http://ceur-ws.org/ $samplehtml
  if [ ! -f $sampletxt ]
  then
    echo "filtering proceeding titles from $samplehtml into $sampletxt"
    cat $samplehtml  | python3 -c 'import html, sys; [print(html.unescape(l), end="") for l in sys.stdin]' | gawk '
BEGIN {
 ignorelines=1
}
/MAINTABLE/ { ignorelines=0; next }
ignorelines { next }
# ignore
# get Vol from e.g.
# <a href="http://ceur-ws.org/Vol-2632/">MIning and REasoning with Legal texts.</a>
/ONLINE:.*<[Aa]\s+(HREF|href)=".*">.*<\/[aA]>/ {
  found=match($0,/[#/]Vol-[0-9]{1,4}/)
  if (found) {
     vol=substr($0, RSTART+1, RLENGTH-1)
     printf("|id=%s\n",vol)
     if (vol=="Vol-1")
       ignorelines=1
  }
  next
}
/<[Aa]\s+(HREF|href)=".*">/,/<\/[aA]>/ { next }
/ftp\/pub/ { next }
/ARCHIVE:/ {next }
/Published on CEUR-WS:/ { next }
/<[aA]\sname/ { next }
/Edited by: / { next }
/<font>/ { next }
/<font size=.?-2.?>/ { next }
/<\/font>/ { next }
/<TD\salign="left"\sbgcolor="#DCDBD7">/ { next }
/#00000/ { next }
/#CCEBC7/ { next }
/#F0D2F5/ { next }
/<\/TD>/ { next }
/<\/td>/ { next }
/<[\/]?table|TABLE>/ { next }
/<[\/]?tr|TR>/ { next }
/--/ { print ""; next }
# FILTER raw html
{
  line=$0
  gsub("<BR>","",line)
  gsub("<br>","",line)
  gsub("<sup>2</sup>","2",line)
  gsub("<(td|TD).bgcolor=.#FFFFFF.>","",line)
  # nbsp
  gsub("Â </TD>","",line)
  printf ("%s",line)
}' | sed  '/^$/d'  > $sampletxt
 else
    echo "$sampletxt already filtered from $samplehtml"
 fi
 wc -l $sampletxt
}

#
# get the text files
#
getText() {
  for source in dblp wikidata
  do
     target=proceedings-$source.txt
     download http://wiki.bitplan.com/images/$target $sampledir/$target
  done
}

#
# download from crossref RESTful API via cursor
#
downloadWithCursor() {
  local l_rows="$1"
  local l_index="$2"
  local l_cursor="$3"
  target=$sampledir/crossref-$l_index.json
  src="https://api.crossref.org/types/proceedings/works?select=event,title,DOI&rows=$l_rows&cursor=$l_cursor"
  download $src $target
}

#
# get Crossref data
# see also https://github.com/TIBHannover/confIDent-dataScraping
#
getCrossRef() {
  rows=1000
  index=1
  totalRows=0
  # force while entry
  total=$rows
  downloadWithCursor $rows $index "*"
  while [ $totalRows -lt $total ]
  do
    target=$sampledir/crossref-$index.json
    status=$(jq '.status' $target | tr -d '"')
    total=$(jq '.message["total-results"]' $target)
    # get and remove quotes from cursor
    cursor=$(jq '.message["next-cursor"]' $target | tr -d '"' | python -c "import urllib.parse;print (urllib.parse.quote(input()))"
    target=$sampledir/crossref-$l_index.json)
    startindex=$(jq '.message.query["start-index"]' $target)
    perpage=$(jq '.message["items-per-page"]' $target)
    index=$[$index+1]
    if [ "$status" == "ok" ]
    then
      totalRows=$[$totalRows+$rows]
    else
      # force while exit
      totalRows=1
      total=0
      # remove invalid
      mv $target $target.err
    fi
    echo "status: $status index: $index $totalRows of $total startindex: $startindex perpage=$perpage cursor:$cursor"
    if [ $totalRows -lt $total ]
    then
      # wait a bit
      sleep 2
      downloadWithCursor $rows $index "$cursor"
    fi
  done
  cat $sampledir/crossref-*.json | jq .message.items[].title | cut -f2 -d'[' | cut -f2 -d'"' | grep -v "]" | tr -s '\n' > $sampledir/proceedings-crossref.txt
}

if [ ! -d $sampledir ]
then
  echo  "creating $sampledir directory"
  mkdir -p $sampledir
else
  echo "$sampledir directory already exists"
fi

getCrossRef
getCEURWS
getText
